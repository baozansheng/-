3.1
设A为第一次选到了中奖门，B为改变选择后选到了中奖门，C为未改变选择后选到了中奖门。
P(A)=1/3,P(A')=2/3,P(B|A)=0,P(C|A)=1,P(B|A')=1,P(C|A')=0,P(B)=P(B|A)*P(A)+P(B|A')*P(A')=2/3,
P(C)=P(C|A')*P(A')+P(C|A)*P(A)=1/3,P(B)>P(C),因此改变选择会提高中奖率，提高1/3。你先任意选择一扇门，每扇门被选中的概率都是1/3，然后基于你的选择这个前提条件，再选择一扇门。倘若你第一次选了一扇没有奖的门，那么，选择一扇没有奖的门的条件概率是1，联合概率是1/3；倘若你第一次选了一扇有奖的门，那么主持人选择一扇没有奖的门的条件概率是1/2，联合概率是1/3*1/2=1/6。所以，不换的中奖概率是1/6+1/6=1/3，换的中奖概率是1/3+1/3=2/3 
3.2
塞尔把房间中的一切都装进头脑，引用中文数据库里的知识，能够流利用中文回答问题，这情形就像物理、化学大部分实验我们都没做过，大部分的词汇的含义都是来自书本中别人的经验，可是我们都觉得自己能够理解这些词汇和相关语句，为什么放在同样做法的AI上就不行呢？有一派人甚至认为，塞尔的思辨来自错误的直觉，就像走进莱布尼茨磨坊一样，所见的直观并不能反映宏观现象的本质。例如考察挥动磁铁时波动的磁场，人们看不见光的发生，但我们不能因此否定光是电磁波。这个思想实验就像以变慢的时钟走进原子尺度一样，让我们看不到光。以微观的尺度和缓慢的过程，来理解“智能”和“理解”的宏观现象，房里人的直觉往往是错的。塞尔对这些批评，用“意识形态的俘虏”一言以蔽之。他说，不明确地指出是什么样的系统，由什么样的机制能导出心灵，就相信了心灵能从系统中“涌现”出来，完全是一种对自己直觉的盲目自信。中文房间的智能更多的像是自动化，收到指令后按照约定俗成的规则（翻译程序）进行反馈。它可以完成一些明确性、无需非实时性数据的指令的回复，比如计算数值，名词释义等等。它在被创造时就已经有了限制，它处理能力是有限的，体现在翻译程序的局限性还有数据支撑。例如我说现在的天气怎么样？它缺少实时数据的支持而无法给我准确的回答。再比如我说：今天我应该在家里休息的。它无法感受从我的表情和语气上获取到我很兴奋的信息，因此不会问我是不是我买的最喜欢的商品给送到了家里。让中文房间变得人工智能，那就将小黑屋变成单面透光玻璃，让屋内的人可以用多种感官同外人交流，并配备传感器或者搜索引擎等获取现实生活中的实时数据。对应着机器要做到人工智能，要给他多种信息获取途径，视觉、听觉、触觉等多种信号输入能帮助它更准确地判断外界要传达的信息，同时要具有学习认知能力，通过学习和反馈来改变自己的原来的思维。
3.3
控制智能家居 
在微软讲师提供的实例程序之上，增加新的场景 
典型场景： 
可以通过语音下达指令。 
支持自然语言的多轮对话，如“打开卧室的灯。厨房的也打开。” 
通过个性化的语音（如学生自己的语音）进行反馈。 
